{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c046f77",
   "metadata": {},
   "source": [
    "### ideas\n",
    "train and validate\n",
    "then tale some custom images, resize and classify them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b148d1",
   "metadata": {},
   "source": [
    "### import and parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import tqdm as notebook_tqdm\n",
    "import csv\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ae0c4",
   "metadata": {},
   "source": [
    "### download from hugging face\n",
    "put into /silver as the images are preprocessed: already properly named and resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a458dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = {\n",
    "    \"pizza\": 76,\n",
    "    \"spaghetti_bolognese\": 90,\n",
    "    \"spaghetti_carbonara\": 91,\n",
    "}\n",
    "N_SPLIT = {\n",
    "    \"train\": 3,\n",
    "    \"validation\": 1,\n",
    "}\n",
    "OUT_ROOT = Path(\"../data/silver\")\n",
    "MAKE_CSV = True\n",
    "EPOCHS = 20\n",
    "IMG_SIZE = 224\n",
    "BATCH = 16\n",
    "BASE_MODEL = \"yolov8n-cls.pt\"   # try yolov8s-cls.pt when you add more data\n",
    "DEVICE = \"cpu\"                 # \"auto\", \"cuda:0\", or \"cpu\"\n",
    "\n",
    "SPLIT_MAP = {\"train\": \"train\", \"validation\": \"val\"}\n",
    "\n",
    "def map_labels():\n",
    "    \"\"\"Map Food-101 label ids <-> names and keep a stable class order.\"\"\"\n",
    "    label_names = load_dataset(\"ethz/food101\", split=\"train\").features[\"label\"].names\n",
    "    id_to_name = {i: n for i, n in enumerate(label_names)}\n",
    "    target_pairs = [(id_to_name[food101_id], food101_id) for _, food101_id in CLASSES.items()]\n",
    "    target_names = [n for n, _ in target_pairs]\n",
    "    target_ids = {i for _, i in target_pairs}\n",
    "    return id_to_name, target_names, target_ids\n",
    "\n",
    "def ensure_dirs(root: Path, target_names):\n",
    "    for split_food101, split_yolo in SPLIT_MAP.items():\n",
    "        for cls in target_names:\n",
    "            (root / split_yolo / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_split(split_food101: str, OUT_ROOT: Path, target_names, target_ids, id_to_name):\n",
    "    \"\"\"\n",
    "    Download images for the given Food-101 split and save in YOLO classification layout:\n",
    "    OUT_ROOT/<train|val>/<class_name>/image.jpg\n",
    "    Optionally writes a labels.csv at split root (filename,label).\n",
    "    \"\"\"\n",
    "    split_yolo = SPLIT_MAP[split_food101]\n",
    "    split_dir = OUT_ROOT / split_yolo\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # counters by class name\n",
    "    saved_counts = {cls: 0 for cls in target_names}\n",
    "    target_per_class = N_SPLIT[split_food101]\n",
    "\n",
    "    ds = load_dataset(\"ethz/food101\", split=split_food101)\n",
    "\n",
    "    rows = []  # optional CSV\n",
    "\n",
    "    for idx, ex in enumerate(ds):\n",
    "        lid = ex[\"label\"]\n",
    "        if lid not in target_ids:\n",
    "            continue\n",
    "\n",
    "        cls_name = id_to_name[lid]\n",
    "        if saved_counts[cls_name] >= target_per_class:\n",
    "            continue\n",
    "\n",
    "        fname = f\"{idx:06d}_{lid:02d}_{cls_name}.jpg\"\n",
    "        out_path = split_dir / cls_name / fname\n",
    "        ex[\"image\"].save(out_path)\n",
    "        saved_counts[cls_name] += 1\n",
    "        if MAKE_CSV:\n",
    "            rows.append([f\"{cls_name}/{fname}\", cls_name])\n",
    "\n",
    "        # stop early when all classes hit the quota\n",
    "        if all(saved_counts[c] >= target_per_class for c in saved_counts):\n",
    "            break\n",
    "\n",
    "    # optional: CSV summary for this split\n",
    "    if MAKE_CSV:\n",
    "        with open(split_dir / \"labels.csv\", \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"filename\", \"label\"])\n",
    "            w.writerows(rows)\n",
    "\n",
    "    print(f\"[{split_food101}] saved counts:\", saved_counts)\n",
    "\n",
    "def write_dataset_yaml(root: Path, target_names):\n",
    "    \"\"\"\n",
    "    Create dataset.yaml for YOLO classification training.\n",
    "    \"\"\"\n",
    "    names_block = \"\\n\".join([f\"  - {n}\" for n in target_names])\n",
    "    yaml_text = f\"\"\"# YOLO classification dataset generated from Food-101\n",
    "path: {root.resolve()}\n",
    "train: train\n",
    "val: val\n",
    "names:\n",
    "{names_block}\n",
    "\"\"\"\n",
    "    (root / \"dataset.yaml\").write_text(yaml_text)\n",
    "    print(f\"dataset.yaml written to: {root / 'dataset.yaml'}\")\n",
    "    print(\"Class order:\", target_names)\n",
    "\n",
    "def prepare_dataset():\n",
    "    id_to_name, target_names, target_ids = map_labels()\n",
    "    ensure_dirs(OUT_ROOT, target_names)\n",
    "    for split in [\"train\", \"validation\"]:\n",
    "        build_split(split, OUT_ROOT, target_names, target_ids, id_to_name)\n",
    "    write_dataset_yaml(OUT_ROOT, target_names)\n",
    "    return (OUT_ROOT / \"dataset.yaml\").resolve()\n",
    "\n",
    "prepare_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba06918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.201 ðŸš€ Python-3.11.13 torch-2.8.0+cu128 CPU (Intel Xeon Platinum 8370C CPU @ 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../data/silver, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/workspaces/marktguru-home-assignment/notebooks/runs/classify/train5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /workspaces/marktguru-home-assignment/data/silver/train... found 9 images in 3 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /workspaces/marktguru-home-assignment/data/silver/val... found 3 images in 3 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=1000 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
      "YOLOv8n-cls summary: 56 layers, 1,442,131 parameters, 1,442,131 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 34.5Â±40.6 MB/s, size: 49.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /workspaces/marktguru-home-assignment/data/silver/train... 9 images, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 1.1Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /workspaces/marktguru-home-assignment/data/silver/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 268.3Â±206.0 MB/s, size: 46.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /workspaces/marktguru-home-assignment/data/silver/val... 3 images, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.1Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /workspaces/marktguru-home-assignment/data/silver/val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/workspaces/marktguru-home-assignment/notebooks/runs/classify/train5\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       1/20         0G      1.137          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.4it/s 0.7s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 8.4it/s 0.1s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       2/20         0G      1.131          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.1it/s 0.5s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 16.8it/s 0.1s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/home/vscode/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 65.3MB/s 0.0s\n",
      "\u001b[K       3/20         0G      1.267          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.7it/s 0.4s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 23.9it/s 0.0s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       4/20         0G      1.145          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.1it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 26.0it/s 0.0s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       5/20         0G      1.147          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.0it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 19.2it/s 0.1s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       6/20         0G      1.074          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.8it/s 0.4s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 26.0it/s 0.0s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       7/20         0G      1.108          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.1it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 26.3it/s 0.0s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       8/20         0G      1.133          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.1it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 26.9it/s 0.0s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       9/20         0G     0.9805          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.0it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 25.1it/s 0.0s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      10/20         0G      1.067          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.9it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 26.6it/s 0.0s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      11/20         0G      0.939          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.1it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 23.6it/s 0.0s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      12/20         0G     0.9405          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.8it/s 0.4s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 17.3it/s 0.1s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      13/20         0G      0.913          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.6it/s 0.6s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 13.2it/s 0.1s\n",
      "                   all          0          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      14/20         0G     0.9024          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.0it/s 0.5s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 17.1it/s 0.1s\n",
      "                   all      0.333          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      15/20         0G     0.8152          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.1it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 26.5it/s 0.0s\n",
      "                   all      0.333          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      16/20         0G     0.8372          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.3it/s 0.4s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 17.0it/s 0.1s\n",
      "                   all      0.333          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      17/20         0G     0.7998          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 2.9it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 20.1it/s 0.0s\n",
      "                   all      0.333          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      18/20         0G     0.8309          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.2it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 25.2it/s 0.0s\n",
      "                   all      0.333          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      19/20         0G     0.8373          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.2it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 25.6it/s 0.0s\n",
      "                   all      0.333          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      20/20         0G     0.7583          9        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 3.3it/s 0.3s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 25.4it/s 0.0s\n",
      "                   all      0.333          1\n",
      "\n",
      "20 epochs completed in 0.003 hours.\n",
      "Optimizer stripped from /workspaces/marktguru-home-assignment/notebooks/runs/classify/train5/weights/last.pt, 3.0MB\n",
      "Optimizer stripped from /workspaces/marktguru-home-assignment/notebooks/runs/classify/train5/weights/best.pt, 3.0MB\n",
      "\n",
      "Validating /workspaces/marktguru-home-assignment/notebooks/runs/classify/train5/weights/best.pt...\n",
      "Ultralytics 8.3.201 ðŸš€ Python-3.11.13 torch-2.8.0+cu128 CPU (Intel Xeon Platinum 8370C CPU @ 2.80GHz)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,438,723 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /workspaces/marktguru-home-assignment/data/silver/train... found 9 images in 3 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /workspaces/marktguru-home-assignment/data/silver/val... found 3 images in 3 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 11.1it/s 0.1s\n",
      "                   all      0.333          1\n",
      "Speed: 0.0ms preprocess, 18.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1m/workspaces/marktguru-home-assignment/notebooks/runs/classify/train5\u001b[0m\n",
      "Ultralytics 8.3.201 ðŸš€ Python-3.11.13 torch-2.8.0+cu128 CPU (Intel Xeon Platinum 8370C CPU @ 2.80GHz)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,438,723 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /workspaces/marktguru-home-assignment/data/silver/train... found 9 images in 3 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /workspaces/marktguru-home-assignment/data/silver/val... found 3 images in 3 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1679.9Â±489.1 MB/s, size: 46.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /workspaces/marktguru-home-assignment/data/silver/val... 3 images, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 8.1Kit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 17.8it/s 0.1s\n",
      "                   all      0.333          1\n",
      "Speed: 0.0ms preprocess, 9.6ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1m/workspaces/marktguru-home-assignment/notebooks/runs/classify/train52\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "../data/val does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     24\u001b[39m     model.predict(\n\u001b[32m     25\u001b[39m         source=Path(DATASET_YAML).parent / \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m         imgsz=IMG_SIZE,\n\u001b[32m     27\u001b[39m         device=DEVICE,\n\u001b[32m     28\u001b[39m         save=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# direct call\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mtrain_eval_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_eval_predict\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m model.val(data=\u001b[38;5;28mstr\u001b[39m(DATASET_YAML), imgsz=IMG_SIZE, device=DEVICE)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Predict on the validation folder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_YAML\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.11.13/lib/python3.11/site-packages/ultralytics/engine/model.py:557\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.11.13/lib/python3.11/site-packages/ultralytics/engine/predictor.py:229\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py:38\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.11.13/lib/python3.11/site-packages/ultralytics/engine/predictor.py:306\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m     \u001b[38;5;28mself\u001b[39m.setup_model(model)\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[32m    305\u001b[39m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_txt:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.11.13/lib/python3.11/site-packages/ultralytics/models/yolo/classify/predict.py:55\u001b[39m, in \u001b[36mClassificationPredictor.setup_source\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msetup_source\u001b[39m(\u001b[38;5;28mself\u001b[39m, source):\n\u001b[32m     54\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Set up source and inference mode and classify transforms.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     updated = (\n\u001b[32m     57\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.model.transforms.transforms[\u001b[32m0\u001b[39m].size != \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.imgsz)\n\u001b[32m     58\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model.model, \u001b[33m\"\u001b[39m\u001b[33mtransforms\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model.model.transforms.transforms[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     60\u001b[39m     )\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.transforms = (\n\u001b[32m     62\u001b[39m         classify_transforms(\u001b[38;5;28mself\u001b[39m.imgsz) \u001b[38;5;28;01mif\u001b[39;00m updated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.pt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.model.transforms\n\u001b[32m     63\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.11.13/lib/python3.11/site-packages/ultralytics/engine/predictor.py:261\u001b[39m, in \u001b[36mBasePredictor.setup_source\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03mSet up source and inference mode.\u001b[39;00m\n\u001b[32m    255\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m \u001b[33;03m        Source for inference.\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28mself\u001b[39m.imgsz = check_imgsz(\u001b[38;5;28mself\u001b[39m.args.imgsz, stride=\u001b[38;5;28mself\u001b[39m.model.stride, min_dim=\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28mself\u001b[39m.dataset = \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28mself\u001b[39m.source_type = \u001b[38;5;28mself\u001b[39m.dataset.source_type\n\u001b[32m    269\u001b[39m long_sequence = (\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m.source_type.stream\n\u001b[32m    271\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.screenshot\n\u001b[32m    272\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset) > \u001b[32m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[32m    273\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33mvideo_flag\u001b[39m\u001b[33m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[32m    274\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.11.13/lib/python3.11/site-packages/ultralytics/data/build.py:307\u001b[39m, in \u001b[36mload_inference_source\u001b[39m\u001b[34m(source, batch, vid_stride, buffer, channels)\u001b[39m\n\u001b[32m    305\u001b[39m     dataset = LoadPilAndNumpy(source, channels=channels)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     dataset = \u001b[43mLoadImagesAndVideos\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Attach source types to the dataset\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;28msetattr\u001b[39m(dataset, \u001b[33m\"\u001b[39m\u001b[33msource_type\u001b[39m\u001b[33m\"\u001b[39m, source_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.11.13/lib/python3.11/site-packages/ultralytics/data/loaders.py:376\u001b[39m, in \u001b[36mLoadImagesAndVideos.__init__\u001b[39m\u001b[34m(self, path, batch, vid_stride, channels)\u001b[39m\n\u001b[32m    374\u001b[39m         files.append(\u001b[38;5;28mstr\u001b[39m((parent / p).absolute()))  \u001b[38;5;66;03m# files (relative to *.txt file parent)\u001b[39;00m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Define files as images or videos\u001b[39;00m\n\u001b[32m    379\u001b[39m images, videos = [], []\n",
      "\u001b[31mFileNotFoundError\u001b[39m: ../data/val does not exist"
     ]
    }
   ],
   "source": [
    "DATASET_YAML = Path(\"../data/silver/\")  # point to your dataset.yaml\n",
    "BASE_MODEL = \"yolov8n-cls.pt\"\n",
    "EPOCHS = 20\n",
    "IMG_SIZE = 224\n",
    "BATCH = 16\n",
    "DEVICE = \"cpu\"   # or \"0\" if you have a GPU\n",
    "\n",
    "def train_eval_predict():\n",
    "    model = YOLO(BASE_MODEL)\n",
    "\n",
    "    # Train\n",
    "    results = model.train(\n",
    "        data=str(DATASET_YAML),\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMG_SIZE,\n",
    "        batch=BATCH,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    # Validate\n",
    "    model.val(data=str(DATASET_YAML), imgsz=IMG_SIZE, device=DEVICE)\n",
    "\n",
    "    # Predict on the validation folder\n",
    "    model.predict(\n",
    "        source=Path(DATASET_YAML).parent / \"val\",\n",
    "        imgsz=IMG_SIZE,\n",
    "        device=DEVICE,\n",
    "        save=True,\n",
    "    )\n",
    "\n",
    "# direct call\n",
    "train_eval_predict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
